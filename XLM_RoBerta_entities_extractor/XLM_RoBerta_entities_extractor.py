"""
NER —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ–∫—Å—Ç–µ
"""

from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import json
from dataclasses import dataclass, asdict
from typing import Optional

from .config import XLM_RoBerta_entities_extractor_config


@dataclass
class Entity:
    """–ù–∞–π–¥–µ–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å"""
    type: str
    text: str
    start: int
    end: int


class XLM_RoBerta_entities_extractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""

    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ("cuda", "cpu" –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–≤—ã–±–æ—Ä–∞)
        """
        if model_path is None:
            config = XLM_RoBerta_entities_extractor_config()
            model_path = config.MODEL_PATH

        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model.eval()

        if device:
            self.device = torch.device(device)
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model.to(self.device)

    def extract(self, text: str) -> list[Entity]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            –°–ø–∏—Å–æ–∫ Entity —Å —Ç–∏–ø–æ–º, —Ç–µ–∫—Å—Ç–æ–º –∏ –ø–æ–∑–∏—Ü–∏—è–º–∏ (start, end)
        """
        if not text or not text.strip():
            return []

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å offset_mapping –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            return_offsets_mapping=True
        )

        offset_mapping = inputs.pop("offset_mapping")[0].tolist()
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(**inputs)

        predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()

        # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        entities = []
        current_entity_type = None
        current_start = None
        current_end = None

        for idx, (pred_id, (start, end)) in enumerate(zip(predictions, offset_mapping)):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            if start == 0 and end == 0:
                continue

            label = self.model.config.id2label[pred_id]

            if label.startswith("B-"):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å
                if current_entity_type is not None:
                    entity_text = text[current_start:current_end]
                    if entity_text.strip():
                        entities.append(Entity(
                            type=current_entity_type,
                            text=entity_text.strip(),
                            start=current_start,
                            end=current_end
                        ))

                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å
                current_entity_type = label[2:]
                current_start = start
                current_end = end

            elif label.startswith("I-") and current_entity_type == label[2:]:
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∏–ø —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
                current_end = end

            else:  # "O" –∏–ª–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞
                if current_entity_type is not None:
                    entity_text = text[current_start:current_end]
                    if entity_text.strip():
                        entities.append(Entity(
                            type=current_entity_type,
                            text=entity_text.strip(),
                            start=current_start,
                            end=current_end
                        ))
                    current_entity_type = None
                    current_start = None
                    current_end = None

        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—É—â–Ω–æ—Å—Ç—å
        if current_entity_type is not None:
            entity_text = text[current_start:current_end]
            if entity_text.strip():
                entities.append(Entity(
                    type=current_entity_type,
                    text=entity_text.strip(),
                    start=current_start,
                    end=current_end
                ))

        return entities

    def extract_to_json(self, text: str) -> dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–ª–æ–≤–∞—Ä—å.

        Returns:
            {
                "text": "–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                "entities": [
                    {"type": "PERSON", "text": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω", "start": 0, "end": 11},
                    ...
                ],
                "entities_by_type": {
                    "PERSON": ["–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω"],
                    "ORG": ["Google"],
                    ...
                }
            }
        """
        entities = self.extract(text)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
        by_type: dict[str, list[str]] = {}
        for e in entities:
            if e.type not in by_type:
                by_type[e.type] = []
            if e.text not in by_type[e.type]:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                by_type[e.type].append(e.text)

        return {
            "text": text,
            "entities": [asdict(e) for e in entities],
            "entities_by_type": by_type
        }

    def anonymize(
        self,
        text: str,
        placeholder_format: str = "[{type}]",
        entity_types: Optional[list[str]] = None
    ) -> dict:
        """
        –ó–∞–º–µ–Ω—è–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            placeholder_format: –§–æ—Ä–º–∞—Ç –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞:
                - "[{type}]" -> [PERSON], [ORG], [DATE]
                - "[REDACTED]" -> –≤—Å—ë –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ [REDACTED]
                - "***" -> –≤—Å—ë –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ ***
            entity_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ –¥–ª—è –∑–∞–º–µ–Ω—ã (None = –≤—Å–µ —Ç–∏–ø—ã)

        Returns:
            {
                "original_text": "...",
                "anonymized_text": "...",
                "replacements": [
                    {"type": ..., "original": ..., "replacement": ..., "start": ..., "end": ...}
                ]
            }
        """
        entities = self.extract(text)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if entity_types:
            entities = [e for e in entities if e.type in entity_types]

        if not entities:
            return {
                "original_text": text,
                "anonymized_text": text,
                "replacements": []
            }

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å –∫–æ–Ω—Ü–∞),
        # —á—Ç–æ–±—ã –∑–∞–º–µ–Ω—ã –Ω–µ —Å–±–∏–≤–∞–ª–∏ –∏–Ω–¥–µ–∫—Å—ã
        entities_sorted = sorted(entities, key=lambda e: e.start, reverse=True)

        anonymized = text
        replacements = []

        for entity in entities_sorted:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
            if "{type}" in placeholder_format:
                placeholder = placeholder_format.format(type=entity.type)
            else:
                placeholder = placeholder_format

            # –ó–∞–º–µ–Ω—è–µ–º
            anonymized = anonymized[:entity.start] + placeholder + anonymized[entity.end:]

            replacements.append({
                "type": entity.type,
                "original": entity.text,
                "replacement": placeholder,
                "start": entity.start,
                "end": entity.end
            })

        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–ª—è —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        replacements.reverse()

        return {
            "original_text": text,
            "anonymized_text": anonymized,
            "replacements": replacements
        }


# ============================================
# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# ============================================

_extractor: Optional[XLM_RoBerta_entities_extractor] = None


def init_extractor(model_path: Optional[str] = None, device: Optional[str] = None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä"""
    global _extractor
    _extractor = XLM_RoBerta_entities_extractor(model_path, device)


def extract_entities(text: str) -> dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–ª–æ–≤–∞—Ä—å —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏
    """
    if _extractor is None:
        raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_extractor()")
    return _extractor.extract_to_json(text)


def anonymize_text(
    text: str,
    placeholder_format: str = "[{type}]",
    entity_types: Optional[list[str]] = None
) -> dict:
    """
    –ê–Ω–æ–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏
        placeholder_format: –§–æ—Ä–º–∞—Ç –∑–∞–º–µ–Ω—ã
        entity_types: –¢–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∑–∞–º–µ–Ω—ã (None = –≤—Å–µ)

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å original_text, anonymized_text, replacements
    """
    if _extractor is None:
        raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_extractor()")
    return _extractor.anonymize(text, placeholder_format, entity_types)


# ============================================
# MAIN - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
# ============================================

if __name__ == "__main__":
    # –ü—É—Ç—å –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    extractor = XLM_RoBerta_entities_extractor()

    test_text = "–Ü–≤–∞–Ω–æ–≤ –Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á - Senior Python Developer –≤ –∫–æ–º–ø–∞–Ω—ñ—ó Google, –ö–∏—ó–≤. –ù–∞–≤–∏—á–∫–∏: Python, Django, PostgreSQL. –î–æ—Å–≤—ñ–¥: 2020-2024."

    print("=" * 60)
    print("üß™ –¢–ï–°–¢ NER –≠–ö–°–¢–†–ê–ö–¢–û–†–ê")
    print("=" * 60)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
    print("\nüìã –°—É—â–Ω–æ—Å—Ç–∏:")
    entities = extractor.extract(test_text)
    for e in entities:
        print(f"   [{e.start}:{e.end}] {e.type}: '{e.text}'")

    # JSON —Ñ–æ—Ä–º–∞—Ç
    print("\nüìÑ JSON:")
    result = extractor.extract_to_json(test_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))

    # –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è
    print("\n" + "=" * 60)
    print("üîí –ê–ù–û–ù–ò–ú–ò–ó–ê–¶–ò–Ø")
    print("=" * 60)

    print(f"\nüìù –û—Ä–∏–≥–∏–Ω–∞–ª:\n   {test_text}")

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    anon = extractor.anonymize(test_text)
    print(f"\nüîí [{{type}}]:\n   {anon['anonymized_text']}")

    # –ï–¥–∏–Ω—ã–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
    anon2 = extractor.anonymize(test_text, placeholder_format="[REDACTED]")
    print(f"\nüîí [REDACTED]:\n   {anon2['anonymized_text']}")

    # –¢–æ–ª—å–∫–æ PERSON
    anon3 = extractor.anonymize(test_text, entity_types=["PER"])
    print(f"\nüîí –¢–æ–ª—å–∫–æ PERSON:\n   {anon3['anonymized_text']}")