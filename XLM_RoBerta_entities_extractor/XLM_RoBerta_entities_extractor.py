"""
NER —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ–∫—Å—Ç–µ
"""

from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import json
from dataclasses import dataclass, asdict
from typing import Optional

from RFC_logging_system.LoggerFactory import get_logger

from .config import XLM_RoBerta_entities_extractor_config


@dataclass
class Entity:
    """–ù–∞–π–¥–µ–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å"""
    type: str
    text: str
    start: int
    end: int


class XLM_RoBerta_entities_extractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""

    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ ("cuda", "cpu" –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–≤—ã–±–æ—Ä–∞)
        """
        self.logger = get_logger("XLM_RoBerta_entities_extractor")
        
        if model_path is None:
            config = XLM_RoBerta_entities_extractor_config()
            model_path = config.MODEL_PATH
            self.logger.debug(f"Using model path from config: {model_path}")
        else:
            self.logger.debug(f"Using provided model path: {model_path}")

        self.logger.info(f"Loading model from: {model_path}")
        try:
            self.model = AutoModelForTokenClassification.from_pretrained(model_path)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model.eval()
            self.logger.info("Model and tokenizer loaded successfully")
        except Exception as e:
            self.logger.error(f"Failed to load model from {model_path}: {str(e)}")
            raise

        if device:
            self.device = torch.device(device)
            self.logger.debug(f"Using specified device: {device}")
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.logger.debug(f"Auto-detected device: {self.device}")

        self.model.to(self.device)
        self.logger.info(f"Model moved to device: {self.device}")

    def extract(self, text: str) -> list[Entity]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            –°–ø–∏—Å–æ–∫ Entity —Å —Ç–∏–ø–æ–º, —Ç–µ–∫—Å—Ç–æ–º –∏ –ø–æ–∑–∏—Ü–∏—è–º–∏ (start, end)
        """
        self.logger.debug(f"Starting entity extraction for text: {text[:100]}...")
        
        if not text or not text.strip():
            self.logger.warning("Empty text provided for entity extraction")
            return []

        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å offset_mapping –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π
            self.logger.debug("Tokenizing input text")
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                return_offsets_mapping=True
            )

            offset_mapping = inputs.pop("offset_mapping")[0].tolist()
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            self.logger.debug("Running model inference")
            with torch.no_grad():
                outputs = self.model(**inputs)

            predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()
            self.logger.debug(f"Model predictions generated, total tokens: {len(predictions)}")

            # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            entities = []
            current_entity_type = None
            current_start = None
            current_end = None

            for idx, (pred_id, (start, end)) in enumerate(zip(predictions, offset_mapping)):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                if start == 0 and end == 0:
                    continue

                label = self.model.config.id2label[pred_id]

                if label.startswith("B-"):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å
                    if current_entity_type is not None:
                        entity_text = text[current_start:current_end]
                        if entity_text.strip():
                            entities.append(Entity(
                                type=current_entity_type,
                                text=entity_text.strip(),
                                start=current_start,
                                end=current_end
                            ))

                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å
                    current_entity_type = label[2:]
                    current_start = start
                    current_end = end

                elif label.startswith("I-") and current_entity_type == label[2:]:
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∏–ø —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
                    current_end = end

                else:  # "O" –∏–ª–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞
                    if current_entity_type is not None:
                        entity_text = text[current_start:current_end]
                        if entity_text.strip():
                            entities.append(Entity(
                                type=current_entity_type,
                                text=entity_text.strip(),
                                start=current_start,
                                end=current_end
                            ))
                        current_entity_type = None
                        current_start = None
                        current_end = None

            # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—É—â–Ω–æ—Å—Ç—å
            if current_entity_type is not None:
                entity_text = text[current_start:current_end]
                if entity_text.strip():
                    entities.append(Entity(
                        type=current_entity_type,
                        text=entity_text.strip(),
                        start=current_start,
                        end=current_end
                    ))

            self.logger.info(f"Entity extraction completed. Found {len(entities)} entities")
            for entity in entities:
                self.logger.debug(f"Found entity: [{entity.start}:{entity.end}] {entity.type}: '{entity.text}'")
            
            return entities
            
        except Exception as e:
            self.logger.error(f"Error during entity extraction: {str(e)}")
            raise

    def extract_to_json(self, text: str) -> dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–ª–æ–≤–∞—Ä—å.

        Returns:
            {
                "text": "–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç",
                "entities": [
                    {"type": "PERSON", "text": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω", "start": 0, "end": 11},
                    ...
                ],
                "entities_by_type": {
                    "PERSON": ["–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω"],
                    "ORG": ["Google"],
                    ...
                }
            }
        """
        self.logger.debug("Starting JSON extraction process")
        
        try:
            entities = self.extract(text)

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
            by_type: dict[str, list[str]] = {}
            for e in entities:
                if e.type not in by_type:
                    by_type[e.type] = []
                if e.text not in by_type[e.type]:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    by_type[e.type].append(e.text)

            result = {
                "text": text,
                "entities": [asdict(e) for e in entities],
                "entities_by_type": by_type
            }
            
            self.logger.debug(f"JSON extraction completed. Total entities: {len(entities)}")
            return result
            
        except Exception as e:
            self.logger.error(f"Error during JSON extraction: {str(e)}")
            raise

    def anonymize(
        self,
        text: str,
        placeholder_format: str = "[{type}]",
        entity_types: Optional[list[str]] = None
    ) -> dict:
        """
        –ó–∞–º–µ–Ω—è–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            placeholder_format: –§–æ—Ä–º–∞—Ç –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞:
                - "[{type}]" -> [PERSON], [ORG], [DATE]
                - "[REDACTED]" -> –≤—Å—ë –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ [REDACTED]
                - "***" -> –≤—Å—ë –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ ***
            entity_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ –¥–ª—è –∑–∞–º–µ–Ω—ã (None = –≤—Å–µ —Ç–∏–ø—ã)

        Returns:
            {
                "original_text": "...",
                "anonymized_text": "...",
                "replacements": [
                    {"type": ..., "original": ..., "replacement": ..., "start": ..., "end": ...}
                ]
            }
        """
        self.logger.debug(f"Starting text anonymization with format: {placeholder_format}")
        
        try:
            entities = self.extract(text)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
            if entity_types:
                self.logger.debug(f"Filtering entities by types: {entity_types}")
                entities = [e for e in entities if e.type in entity_types]

            if not entities:
                self.logger.debug("No entities found for anonymization")
                return {
                    "original_text": text,
                    "anonymized_text": text,
                    "replacements": []
                }

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å –∫–æ–Ω—Ü–∞),
            # —á—Ç–æ–±—ã –∑–∞–º–µ–Ω—ã –Ω–µ —Å–±–∏–≤–∞–ª–∏ –∏–Ω–¥–µ–∫—Å—ã
            entities_sorted = sorted(entities, key=lambda e: e.start, reverse=True)
            self.logger.debug(f"Found {len(entities)} entities to anonymize")

            anonymized = text
            replacements = []

            for entity in entities_sorted:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
                if "{type}" in placeholder_format:
                    placeholder = placeholder_format.format(type=entity.type)
                else:
                    placeholder = placeholder_format

                # –ó–∞–º–µ–Ω—è–µ–º
                anonymized = anonymized[:entity.start] + placeholder + anonymized[entity.end:]

                replacements.append({
                    "type": entity.type,
                    "original": entity.text,
                    "replacement": placeholder,
                    "start": entity.start,
                    "end": entity.end
                })

                self.logger.debug(f"Replaced entity: '{entity.text}' -> '{placeholder}' at [{entity.start}:{entity.end}]")

            # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–ª—è —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
            replacements.reverse()

            self.logger.info(f"Anonymization completed. {len(replacements)} replacements made")
            return {
                "original_text": text,
                "anonymized_text": anonymized,
                "replacements": replacements
            }
            
        except Exception as e:
            self.logger.error(f"Error during text anonymization: {str(e)}")
            raise


# ============================================
# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# ============================================

_extractor: Optional[XLM_RoBerta_entities_extractor] = None


def init_extractor(model_path: Optional[str] = None, device: Optional[str] = None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä"""
    global _extractor
    logger = get_logger("XLM_RoBerta_entities_extractor")
    
    logger.info("Initializing global extractor")
    try:
        _extractor = XLM_RoBerta_entities_extractor(model_path, device)
        logger.info("Global extractor initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize global extractor: {str(e)}")
        raise


def extract_entities(text: str) -> dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–ª–æ–≤–∞—Ä—å —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏
    """
    logger = get_logger("XLM_RoBerta_entities_extractor")
    
    if _extractor is None:
        logger.error("Global extractor not initialized. Call init_extractor() first")
        raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_extractor()")
    
    logger.debug("Extracting entities using global extractor")
    try:
        result = _extractor.extract_to_json(text)
        logger.debug("Entity extraction completed successfully")
        return result
    except Exception as e:
        logger.error(f"Error during entity extraction: {str(e)}")
        raise


def anonymize_text(
    text: str,
    placeholder_format: str = "[{type}]",
    entity_types: Optional[list[str]] = None
) -> dict:
    """
    –ê–Ω–æ–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏
        placeholder_format: –§–æ—Ä–º–∞—Ç –∑–∞–º–µ–Ω—ã
        entity_types: –¢–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –∑–∞–º–µ–Ω—ã (None = –≤—Å–µ)

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å original_text, anonymized_text, replacements
    """
    logger = get_logger("XLM_RoBerta_entities_extractor")
    
    if _extractor is None:
        logger.error("Global extractor not initialized. Call init_extractor() first")
        raise RuntimeError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_extractor()")
    
    logger.debug(f"Anonymizing text with format: {placeholder_format}")
    try:
        result = _extractor.anonymize(text, placeholder_format, entity_types)
        logger.debug("Text anonymization completed successfully")
        return result
    except Exception as e:
        logger.error(f"Error during text anonymization: {str(e)}")
        raise


# ============================================
# MAIN - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
# ============================================

if __name__ == "__main__":
    logger = get_logger("XLM_RoBerta_entities_extractor")
    logger.info("Starting XLM_RoBerta_entities_extractor demonstration")
    # –ü—É—Ç—å –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    try:
        extractor = XLM_RoBerta_entities_extractor()

        test_text = "–Ü–≤–∞–Ω–æ–≤ –Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á - Senior Python Developer –≤ –∫–æ–º–ø–∞–Ω—ñ—ó Google, –ö–∏—ó–≤. –ù–∞–≤–∏—á–∫–∏: Python, Django, PostgreSQL. –î–æ—Å–≤—ñ–¥: 2020-2024."

        print("=" * 60)
        print("üß™ –¢–ï–°–¢ NER –≠–ö–°–¢–†–ê–ö–¢–û–†–ê")
        print("=" * 60)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        print("\nüìã –°—É—â–Ω–æ—Å—Ç–∏:")
        entities = extractor.extract(test_text)
        for e in entities:
            print(f"   [{e.start}:{e.end}] {e.type}: '{e.text}'")

        # JSON —Ñ–æ—Ä–º–∞—Ç
        print("\nüìÑ JSON:")
        result = extractor.extract_to_json(test_text)
        print(json.dumps(result, ensure_ascii=False, indent=2))

        # –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è
        print("\n" + "=" * 60)
        print("üîí –ê–ù–û–ù–ò–ú–ò–ó–ê–¶–ò–Ø")
        print("=" * 60)

        print(f"\nüìù –û—Ä–∏–≥–∏–Ω–∞–ª:\n   {test_text}")

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        anon = extractor.anonymize(test_text)
        print(f"\nüîí [{{type}}]:\n   {anon['anonymized_text']}")

        # –ï–¥–∏–Ω—ã–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
        anon2 = extractor.anonymize(test_text, placeholder_format="[REDACTED]")
        print(f"\nüîí [REDACTED]:\n   {anon2['anonymized_text']}")

        # –¢–æ–ª—å–∫–æ PERSON
        anon3 = extractor.anonymize(test_text, entity_types=["PER"])
        print(f"\nüîí –¢–æ–ª—å–∫–æ PERSON:\n   {anon3['anonymized_text']}")
        
        logger.info("Demonstration completed successfully")
        
    except Exception as e:
        logger.error(f"Error during demonstration: {str(e)}")
        raise