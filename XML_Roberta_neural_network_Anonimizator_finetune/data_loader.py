import os
from pathlib import Path
from typing import List, Tuple, Dict
from datasets import Dataset
from sklearn.model_selection import train_test_split


def parse_conll(file_path: str) -> Tuple[List[List[str]], List[List[str]]]:
    """–ß–∏—Ç–∞–µ—Ç CoNLL —Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (sentences, labels)."""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    sentences, labels = [], []
    tokens, tags = [], []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith('#'): continue

            if not line:
                if tokens:
                    sentences.append(tokens)
                    labels.append(tags)
                    tokens, tags = [], []
                continue

            parts = line.split('\t') if '\t' in line else line.split()
            if len(parts) >= 2:
                tokens.append(parts[0])
                tags.append(parts[-1])

        if tokens:  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±—É—Ñ–µ—Ä
            sentences.append(tokens)
            labels.append(tags)

    return sentences, labels


def split_and_save_data(source_path: str, train_path: str, val_path: str, split_ratio=0.2):
    """–î–µ–ª–∏—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ train/val –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫."""
    print(f"üì¶ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_path}...")

    # –°–Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–º, —á—Ç–æ–±—ã –¥–µ–ª–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –∞ –Ω–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
    sentences, labels = parse_conll(source_path)

    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏
    text_examples = []
    for sent, lab in zip(sentences, labels):
        lines = [f"{t} {l}" for t, l in zip(sent, lab)]
        text_examples.append("\n".join(lines))

    train_ex, val_ex = train_test_split(text_examples, test_size=split_ratio, random_state=42)

    Path(os.path.dirname(train_path)).mkdir(parents=True, exist_ok=True)

    with open(train_path, 'w', encoding='utf-8') as f:
        f.write('\n\n'.join(train_ex))
    with open(val_path, 'w', encoding='utf-8') as f:
        f.write('\n\n'.join(val_ex))

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã: Train ({len(train_ex)}), Val ({len(val_ex)})")


def create_dataset(file_path: str, label2id: Dict[str, int] = None) -> Tuple[Dataset, Dict[str, int]]:
    """–°–æ–∑–¥–∞–µ—Ç HuggingFace Dataset –∏ —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫."""
    sentences, labels = parse_conll(file_path)

    # –ï—Å–ª–∏ –º–∞–ø–ø–∏–Ω–≥ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    if label2id is None:
        unique_tags = sorted(set(tag for seq in labels for tag in seq))
        label2id = {tag: i for i, tag in enumerate(unique_tags)}

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–µ–≥–æ–≤ –≤ ID
    label_ids = []
    for seq in labels:
        # get('O', 0) –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–µ–≥–æ–≤
        ids = [label2id.get(t, label2id.get('O', 0)) for t in seq]
        label_ids.append(ids)

    dataset = Dataset.from_dict({"tokens": sentences, "ner_tags": label_ids})
    return dataset, label2id