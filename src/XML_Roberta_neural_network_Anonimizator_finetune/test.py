"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π NER –º–æ–¥–µ–ª–∏
"""

from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch


def load_model(model_path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    model = AutoModelForTokenClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model.eval()

    # GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
    print(f"   Device: {device}")
    print(f"   Labels: {list(model.config.id2label.values())}")

    return model, tokenizer, device


def predict(text: str, model, tokenizer, device) -> list[dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    predictions = torch.argmax(outputs.logits, dim=2)
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    labels = [model.config.id2label[p.item()] for p in predictions[0]]

    entities = []
    current_entity = None
    current_tokens = []

    for token, label in zip(tokens, labels):
        if token in ["<s>", "</s>", "<pad>"]:
            continue

        if label.startswith("B-"):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å
            if current_entity:
                text_value = tokenizer.convert_tokens_to_string(current_tokens).strip()
                if text_value:
                    entities.append({"type": current_entity, "text": text_value})

            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
            current_entity = label[2:]
            current_tokens = [token]

        elif label.startswith("I-") and current_entity == label[2:]:  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —Ç–∏–ø —Å–æ–≤–ø–∞–¥–∞–µ—Ç
            current_tokens.append(token)

        else:  # "O" –∏–ª–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞
            if current_entity:
                text_value = tokenizer.convert_tokens_to_string(current_tokens).strip()
                if text_value:
                    entities.append({"type": current_entity, "text": text_value})
                current_entity = None
                current_tokens = []

    # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—É—â–Ω–æ—Å—Ç—å
    if current_entity:
        text_value = tokenizer.convert_tokens_to_string(current_tokens).strip()
        if text_value:
            entities.append({"type": current_entity, "text": text_value})

    return entities


def print_entities(entities: list[dict]):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Å—É—â–Ω–æ—Å—Ç–∏"""
    if not entities:
        print("   –°—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
    by_type = {}
    for e in entities:
        t = e["type"]
        if t not in by_type:
            by_type[t] = []
        by_type[t].append(e["text"])

    for entity_type, values in sorted(by_type.items()):
        print(f"   {entity_type}: {', '.join(values)}")


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":

    # –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    MODEL_PATH = "XML_Roberta_neural_network_Anonimizator_finetune/ner_model_output"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer, device = load_model(MODEL_PATH)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_texts = [
        "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á - Senior Python Developer –≤ –∫–æ–º–ø–∞–Ω–∏–∏ Google, –ö–∏–µ–≤. –ù–∞–≤—ã–∫–∏: Python, Django, PostgreSQL. –û–ø—ã—Ç: 2020-2024.",

        "Bezkorovainy Mykyta worked as Angular Developer at SmartFox Pro from March 2022. Skills: Angular, TypeScript, RxJS.",

        "–®–µ–≤—á–µ–Ω–∫–æ –¢–∞—Ä–∞—Å, Junior Data Scientist, SoftServe, –õ—å–≤—ñ–≤. Python, TensorFlow, Pandas. 01/2023 - present.",

        "SQL Developer. Python, Django, Flask",
    ]

    print("\n" + "=" * 60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï NER –ú–û–î–ï–õ–ò")
    print("=" * 60)

    for i, text in enumerate(test_texts, 1):
        print(f"\nüìù –¢–µ–∫—Å—Ç {i}:")
        print(f"   {text[:80]}..." if len(text) > 80 else f"   {text}")

        entities = predict(text, model, tokenizer, device)

        print(f"\nüè∑Ô∏è –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
        print_entities(entities)
        print("-" * 60)

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\nüí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (–≤–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):")
    while True:
        try:
            user_text = input("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç: ").strip()
            if user_text.lower() == "exit":
                break
            if not user_text:
                continue

            entities = predict(user_text, model, tokenizer, device)
            print("\nüè∑Ô∏è –°—É—â–Ω–æ—Å—Ç–∏:")
            print_entities(entities)

        except KeyboardInterrupt:
            break

    print("\nüëã –ì–æ—Ç–æ–≤–æ!")